{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgHKTxBHa4sD9szfKnTTId"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install beautifulsoup4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWbAEB7ztgDp","executionInfo":{"status":"ok","timestamp":1712936668412,"user_tz":240,"elapsed":20039,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"outputId":"aeb986e7-7cf9-4b47-a2cf-6e50cbb04f24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"]}]},{"cell_type":"code","source":["from enum import Enum\n","from bs4 import BeautifulSoup"],"metadata":{"id":"4YVsdVjm_5Ce","executionInfo":{"status":"ok","timestamp":1712936672553,"user_tz":240,"elapsed":4144,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"58c8c920-c882-4705-c287-398fdd435255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["class ScrapeType(Enum):\n","  VIDEO = 1,\n","  APOLOGY = 2,\n","  NTU = 3,\n","  GLOSBE = 4,"],"metadata":{"id":"WCe71aznpHDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fb8u4RPs980m"},"outputs":[],"source":["# Scrapes parallel video subtitles from \"https://ailt.ilrdf.org.tw/colloquial/index\"\n","class HTMLScraper:\n","  def __init__(self, props: dict = {}):\n","    self.props = props\n","\n","  def init_soup(self, html: str):\n","    self.soup = BeautifulSoup(html)\n","\n","  # Returns a list of lists pairing source lang subtitles with target lang subtitles\n","  def parallel_scrape(self, source_class: str, target_class: str):\n","    sources = self.soup.find_all('div', class_=source_class) #translation_ind\n","    targets = self.soup.find_all('div', class_=target_class)  #translation_zh\n","\n","    pairs = []\n","    for source_div, target_div in zip(sources, targets):\n","      source_spans = [span.text for span in source_div.find_all('span', class_='ind_dictionary')]\n","      source_line = ' '.join(source_spans)\n","      target_line = target_div.get_text(strip=True)\n","      pairs.append([source_line, target_line])\n","\n","    return pairs"]},{"cell_type":"code","source":["# Scrapes parallel sentences from presidential apologies or any text file.\n","class TextFileScraper:\n","  def __init__(self, props: dict = {}):\n","    self.props = props\n","\n","  def set_doc_locations(self, d1: str, d2: str):\n","    self.source_loc = d1\n","    self.target_loc = d2\n","\n","  def parallel_scrape(self):\n","    with open(self.source_loc, 'r', encoding='utf-8') as file:\n","      source_text = file.read()\n","    with open(self.target_loc, 'r', encoding='utf-8') as file:\n","      target_text = file.read()\n","\n","    source_sents = source_text.split(\"\\n\")\n","    target_sents = target_text.split(\"\\n\")\n","    min_length = min(len(source_sents), len(target_sents)) # Min length sentence alignment - may change\n","    source_sents = source_sents[:min_length]\n","    target_sents = target_sents[:min_length]\n","\n","    pairs = []\n","    for source_sent, target_sent in zip(source_sents, target_sents):\n","        pairs.append([source_sent.strip() + '\\n', target_sent.strip() + '\\n'])\n","\n","    return pairs\n"],"metadata":{"id":"VTXW5_F4Dd18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Factory method for creating scrapers\n","def getScraper(scrapeType : ScrapeType):\n","    scrapers = {\n","        ScrapeType.VIDEO: HTMLScraper,\n","        ScrapeType.APOLOGY: TextFileScraper,\n","        ScrapeType.NTU: TextFileScraper,\n","        ScrapeType.GLOSBE: HTMLScraper\n","    }\n","\n","    return scrapers[scrapeType]()"],"metadata":{"id":"HwIqF3J6ENFo"},"execution_count":null,"outputs":[]}]}