{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrqfaBeA6+bvqHCzRCeFUF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install nltk\n","!pip install sacremoses\n","!pip install pyspellchecker\n","!pip install contractions"],"metadata":{"id":"A8Sd6SakyzkX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713913469888,"user_tz":240,"elapsed":28648,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"outputId":"25ae0fb6-9248-472b-c623-01d5160228cd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.2)\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.1.1\n","Collecting pyspellchecker\n","  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.8.1\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"]}]},{"cell_type":"code","source":["from spellchecker import SpellChecker\n","from sacremoses import MosesTruecaser, MosesTokenizer, MosesPunctNormalizer\n","import contractions\n","from enum import Enum\n","import re\n","import statistics as stats\n","import math\n","import nltk\n","nltk.download('punkt')"],"metadata":{"id":"ilfQoZ_oLADW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713913473835,"user_tz":240,"elapsed":3955,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"outputId":"1346153c-a2ce-4222-8a37-8f4c9948240d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["class PreprocessStrategies(Enum):\n","  NORMALIZE_PUNCT = 1\n","  REMOVE_PUNCT = 2\n","  STOPWORDS = 3\n","  CONTRACTIONS = 4\n","  SPELLCHECK = 5\n","  TRUECASE = 6"],"metadata":{"id":"exf-UHlG89dH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBLsBaT3OOGP"},"outputs":[],"source":["class Preprocessor:\n","  def __init__(self):\n","    self.spell = SpellChecker()\n","    self.normalizer = MosesPunctNormalizer()\n","    self.strategies = []\n","    self.text = []\n","\n","  def __remove_punctuation(self, text):\n","    return re.sub(r'(?<!\\w)[^\\s\\w]|(?!\\w)[^\\s\\w]', '', text)\n","\n","\n","  def __expand_contractions(self, text):\n","    return contractions.fix(text)\n","\n","\n","  def __spellcheck(self, text):\n","    out = []\n","    for word in text.split(' '):\n","      if not word or any(chr.isdigit() for chr in word): continue\n","      corrected = word[0].isupper() and word or self.spell.correction(word)\n","      corrected = corrected is not None and corrected or word\n","      out.append(corrected)\n","\n","    return \" \".join(out)\n","\n","\n","  def __normalize_punctuation(self, text):\n","    return self.normalizer.normalize(text)\n","\n","\n","  def __train_truecasers(self):\n","    tokenizer = MosesTokenizer()\n","    truecaser_indi = MosesTruecaser()\n","    truecaser_eng = MosesTruecaser()\n","\n","    tokenized_indi = [tokenizer.tokenize(line[0]) for line in self.text]\n","    tokenized_eng = [tokenizer.tokenize(line[1]) for line in self.text]\n","    truecaser_indi.train(tokenized_indi, save_to='/content/drive/MyDrive/MTApplication/models/indi.truecasemodel')\n","    truecaser_eng.train(tokenized_eng, save_to='/content/drive/MyDrive/MTApplication/models/eng.truecasemodel')\n","\n","    self.truecaser_indi = MosesTruecaser('/content/drive/MyDrive/MTApplication/models/indi.truecasemodel')\n","    self.truecaser_eng = MosesTruecaser('/content/drive/MyDrive/MTApplication/models/eng.truecasemodel')\n","\n","\n","  def __truecase(self, source, target):\n","    return (\" \".join(self.truecaser_indi.truecase(source)),\n","            \" \".join(self.truecaser_eng.truecase(target)))\n","\n","\n","  def __normalize_spaces(self, text):\n","    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text) # Separate improperly joined words\n","    text = re.sub(r'\\s+', ' ', text) # Collapse spaces\n","    return text\n","\n","\n","  def __remove_paranthetical(self, text):\n","    return re.sub(r'\\([^)]*\\)', '', text)\n","\n","\n","  def __handle_one_to_many(self, source, target):\n","    if \";\" not in target:\n","      return (source, target)\n","\n","    source_split = source.split(' ')\n","    target_split = target.split(';')\n","\n","    if len(source_split) != 1:\n","      return (source, target)\n","\n","    return (source, min(target_split, key=len).lower())\n","\n","\n","  def __purge_non_english(self, text):\n","    return re.sub(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\", '', text)\n","\n","\n","  def __filter(self, source, target):\n","    source_split = [w for w in source.split(' ') if w != \"\"]\n","    target_split = [w for w in target.split(' ') if w != \"\"]\n","    diff = math.pow((abs(len(source) - len(target))), 1/3)\n","\n","    if not source_split or not target_split:\n","      return True\n","\n","    if \"no record\" in source.lower():\n","      return True\n","\n","    if \"no chinese record\" in target.lower():\n","      return True\n","\n","    if len(source_split) > 150 or len(target_split) > 150:\n","      return True\n","\n","    if len(max(source_split, key=len)) > 20 or len(max(target_split, key=len)) > 20:\n","      return True\n","\n","    if diff < self.f_heuristic[0] and diff != 0 or diff > self.f_heuristic[1]:\n","      return True\n","\n","    return False\n","\n","\n","  def __dedupe(self, text):\n","    toks = nltk.sent_tokenize(text)\n","    deduped = list(dict.fromkeys(toks))\n","    return \" \".join([s for s in deduped])\n","\n","\n","  def __replace_seq(self, text: str, old: str, new: str):\n","    return text.replace(old, new)\n","\n","\n","  def __calc_fertility_heuristic(self):\n","    diffs = []\n","\n","    for line in self.text:\n","      src = line[0]\n","      tgt = line[1]\n","      diff = math.pow((abs(len(src) - len(tgt))), 1/3)\n","      diffs.append(diff)\n","\n","    self.f_heuristic = (stats.mean(diffs) - stats.stdev(diffs)*2,\n","                               stats.mean(diffs) + stats.stdev(diffs)*2)\n","\n","\n","  def partition(self):\n","    size = len(self.text)\n","    step = size // 4\n","    result = [(i, i + step) for i in range(0, size, step)]\n","    result[-1] = (result[-1][0], size)\n","    return result\n","\n","  def set_strategy(self, strategies: list[PreprocessStrategies]):\n","    self.strategies = list(set(strategies)).sort(key=lambda x: x.value)\n","\n","\n","  def load_text(self, text: list[dict]):\n","    self.text = [(item['Amis'], item['English']) for item in text]\n","    self.__train_truecasers()\n","    self.__calc_fertility_heuristic()\n","\n","\n","  def get_text(self):\n","    return self.text\n","\n","\n","  def process(self, partition: tuple[int, int]):\n","    processed = set()\n","\n","    for i in range(partition[0], partition[1]):\n","      print(f\"\\r{i+1}/{len(self.text)}\", end='')\n","      pair = self.text[i]\n","      indigenous = pair[0]\n","      english = pair[1]\n","      indigenous = self.__normalize_punctuation(indigenous)\n","      english = self.__normalize_punctuation(english)\n","      english = self.__purge_non_english(english)\n","      english = self.__replace_seq(english, \"sth.\", \"something\")\n","      english = self.__replace_seq(english, \"sb.\", \"somebody\")\n","      english = self.__replace_seq(english, \"-\", \" \")\n","      english = self.__expand_contractions(english)\n","      (indigenous, english) = self.__truecase(indigenous, english)\n","      english = self.__dedupe(english)\n","      english = self.__remove_paranthetical(english)\n","      indigenous = self.__remove_paranthetical(indigenous)\n","      (indigenous, english) = self.__handle_one_to_many(indigenous, english)\n","      indigenous = self.__remove_punctuation(indigenous)\n","      english = self.__remove_punctuation(english)\n","      english = self.__normalize_spaces(english)\n","      indigenous = self.__normalize_spaces(indigenous)\n","      english = self.__spellcheck(english)\n","      indigenous = self.__replace_seq(indigenous, 'o', 'u')\n","      if self.__filter(indigenous, english): continue\n","      processed.add((indigenous, english))\n","\n","    return list(processed)"]}]}