{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Written by Dylan Leddy"],"metadata":{"id":"-eG4AIapN6xh"}},{"cell_type":"markdown","source":["The Preprocessor handles all data augmentation prior to training"],"metadata":{"id":"gHrOOpssJfmU"}},{"cell_type":"code","source":["!pip install nltk\n","!pip install sacremoses\n","!pip install pyspellchecker\n","!pip install contractions"],"metadata":{"id":"A8Sd6SakyzkX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714583577703,"user_tz":420,"elapsed":46323,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"outputId":"41b4d93a-0dbd-4325-8809-5f4b3c3cf4d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.2)\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.1.1\n","Collecting pyspellchecker\n","  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.8.1\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"]}]},{"cell_type":"code","source":["from spellchecker import SpellChecker\n","from sacremoses import MosesTruecaser, MosesTokenizer, MosesPunctNormalizer\n","import contractions\n","from enum import Enum\n","import re\n","import statistics as stats\n","import math\n","import nltk\n","import os\n","nltk.download('punkt')"],"metadata":{"id":"ilfQoZ_oLADW","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"error","timestamp":1714599813819,"user_tz":420,"elapsed":6,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"outputId":"38736427-9243-4ce3-f570-04793939edb9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'spellchecker'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-97b9b5490f9e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msacremoses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMosesTruecaser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMosesTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMosesPunctNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["We implement a **strategy pattern** below, providing the client with control over what transformations are applied.\n","\n","Note that the transformations are **sorted** - the order of operations matters for yielding quality text."],"metadata":{"id":"L37C6y0JJoMD"}},{"cell_type":"code","source":["# Condenses punctuation to a set of common characters\n","class NormalizePunct:\n","  def __init__(self):\n","    self.sortKey = 1\n","    self.normalizer = MosesPunctNormalizer()\n","\n","  def execute(self, text: str):\n","    return self.normalizer.normalize(text)\n","\n","\n","# Attempts to separate incorrectly joined words and removes excess white space.\n","class NormalizeSpaces:\n","  def __init__(self):\n","    self.sortKey = 10\n","\n","  def execute(self, text: str):\n","    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text\n","\n","\n","# Removes all punctuation not enclosed by 2 alphanumerics\n","class RemovePunct:\n","  def __init__(self):\n","    self.sortKey = 9\n","\n","  def execute(self, text):\n","    return re.sub(r'(?<!\\w)[^\\s\\w]|(?!\\w)[^\\s\\w]', '', text)\n","\n","\n","# E.g. expanding \"I'm\" to \"I am\"\n","class ExpandContractions:\n","  def __init__(self):\n","    self.sortKey = 4\n","\n","  def execute(self, text: str):\n","    return contractions.fix(text)\n","\n","\n","# Careful, this may output bad corrections - only use for english\n","class Spellcheck:\n","  def __init__(self):\n","    self.spell = SpellChecker()\n","    self.sortKey = 11\n","\n","  def execute(self, text: str):\n","    out = []\n","    for word in text.split(' '):\n","      if not word or any(chr.isdigit() for chr in word): continue\n","      corrected = word[0].isupper() and word or self.spell.correction(word)\n","      corrected = corrected is not None and corrected or word\n","      out.append(corrected)\n","\n","    return \" \".join(out)\n","\n","\n","# Attempts to retain capitalize of proper nouns while lowercasing others\n","class Truecase:\n","  def __init__(self):\n","    self.sortKey = 5\n","\n","  def execute(self, truecaser: object, text: str):\n","    return \" \".join(truecaser.truecase(text))\n","\n","\n","class PurgeNonEnglish:\n","  def __init__(self):\n","    self.sortKey = 2\n","\n","  def execute(self, text: str):\n","    return re.sub(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\", '', text)\n","\n","\n","class ReplaceSeq:\n","  def __init__(self, old: str, new: str):\n","    self.old = old\n","    self.new = new\n","    self.sortKey = 3\n","\n","  def execute(self, text: str):\n","    return text.replace(self.old, self.new)\n","\n","\n","# Intra-field deduplication of sentences\n","class Dedupe:\n","  def __init__(self):\n","    self.sortKey = 6\n","\n","  def execute(self, text: str):\n","    toks = nltk.sent_tokenize(text)\n","    deduped = list(dict.fromkeys(toks))\n","    return \" \".join([s for s in deduped])\n","\n","\n","# Removes everything enclosed by parenthesis\n","class RemoveParenthetical:\n","  def __init__(self):\n","    self.sortKey = 7\n","\n","  def execute(self, text: str):\n","    return re.sub(r'\\([^)]*\\)', '', text)\n","\n","\n","# Transforms one-to-many translations to one-to-one\n","# These mappings are identified by a \";\" separator, which is unique to our dataset\n","# This may be expanded to include logical operators (src -> word1 or word2)\n","class HandleOneToMany:\n","  def __init__(self):\n","    self.sortKey = 8\n","\n","  def execute(self, target: str, source: str):\n","    if \";\" not in target:\n","      return target\n","\n","    source_split = source.split(' ')\n","    target_split = target.split(';')\n","\n","    if len(source_split) != 1:\n","      return target\n","\n","    return min(target_split, key=len).lower()"],"metadata":{"id":"7tLM9LFQAv7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The context for one side of our parallel corpus\n","class Context:\n","  TRUECASER_SAVEPATH = '/content/drive/MyDrive/MTApplication/models/'\n","\n","  def __init__(self, language: str, corpus: list[dict]):\n","    self.text = [item[language] for item in corpus]\n","    self.language = language\n","\n","  def __train_truecasers(self, text: list[str], save_to: str):\n","    if not os.path.exists(save_to + '.truecasemodel'):\n","      tokenizer = MosesTokenizer()\n","      truecaser = MosesTruecaser()\n","      tokenized = [tokenizer.tokenize(line) for line in self.text]\n","      truecaser.train(tokenized, save_to=save_to + '.truecasemodel')\n","\n","    self.truecaser = MosesTruecaser(save_to + '.truecasemodel')\n","\n","  def get_text(self):\n","    return self.text\n","\n","  def get_line(self, i: int):\n","    return self.text[i]\n","\n","  # We provide a list of transforms\n","  def set_strategy(self, strategies: list[object]):\n","    self.strategies = sorted(strategies, key=lambda x: x.sortKey)\n","    for strategy in strategies:\n","      if isinstance(strategy, Truecase):\n","        self.__train_truecasers(\n","          self.text,\n","          self.TRUECASER_SAVEPATH + self.language\n","        )\n","        break\n","\n","  # Carry out transformations on each line\n","  def process(self, i: int, other: str):\n","    txt = self.text[i]\n","    for strategy in self.strategies:\n","      if isinstance(strategy, HandleOneToMany):\n","        txt = strategy.execute(txt, other)\n","      elif isinstance(strategy, Truecase):\n","        txt = strategy.execute(self.truecaser, txt)\n","      else:\n","        txt = strategy.execute(txt)\n","    return txt"],"metadata":{"id":"exf-UHlG89dH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBLsBaT3OOGP"},"outputs":[],"source":["# Pass in src/tgt contexts and process\n","class Preprocessor:\n","  def __init__(self, src_context: Context, tgt_context: Context):\n","    self.src_context = src_context\n","    self.tgt_context = tgt_context\n","    self.lines = min(len(src_context.get_text()), len(tgt_context.get_text()))\n","    self.__calc_fertility_heuristic()\n","\n","  # Very important! Discards incomplete translations or pairs w/bad size ratios\n","  def __filter(self, source: str, target: str):\n","    source_split = [w for w in source.split(' ') if w != \"\"]\n","    target_split = [w for w in target.split(' ') if w != \"\"]\n","    diff = math.pow((abs(len(source) - len(target))), 1/3)\n","\n","    if not source_split or not target_split:\n","      return True\n","\n","    if \"no record\" in source.lower():\n","      return True\n","\n","    if \"no chinese record\" in target.lower():\n","      return True\n","\n","    if len(source_split) > 150 or len(target_split) > 150: # Num word threshold 150\n","      return True\n","\n","    if len(max(source_split, key=len)) > 20 or len(max(target_split, key=len)) > 20: # Max word length <= 20\n","      return True\n","\n","    if diff < self.f_heuristic[0] and diff != 0 or diff > self.f_heuristic[1]: # Fertility (char ratios)\n","      return True\n","\n","    return False\n","\n","  # We calculate the average absolute difference in string lengths\n","  # Apply a pow transformation to make data normal\n","  # Discard pairs w/differences outside 2nd stdev\n","  def __calc_fertility_heuristic(self):\n","    diffs = []\n","\n","    for line in range(self.lines):\n","      src = self.src_context.get_line(line)\n","      tgt = self.tgt_context.get_line(line)\n","      diff = math.pow((abs(len(src) - len(tgt))), 1/3)\n","      diffs.append(diff)\n","\n","    self.f_heuristic = (stats.mean(diffs) - stats.stdev(diffs)*2,\n","                               stats.mean(diffs) + stats.stdev(diffs)*2)\n","\n","  def get_total_lines(self):\n","    return self.lines\n","\n","  # May process and save in batches if need be\n","  def partition(self):\n","    size = self.lines\n","    step = size // 4\n","    result = [(i, i + step) for i in range(0, size, step)]\n","    result[-1] = (result[-1][0], size)\n","    return result\n","\n","  def process(self, partition: tuple[int, int]) -> list[tuple[str, str]]:\n","    processed = set()\n","\n","    for i in range(partition[0], partition[1]):\n","      print(f\"\\r{i+1}/{self.lines}\", end='')\n","\n","      src_ctx, tgt_ctx = self.src_context, self.tgt_context\n","      src_txt, tgt_txt = src_ctx.get_line(i), tgt_ctx.get_line(i)\n","\n","      src_txt_processed = src_ctx.process(i, tgt_txt)\n","      tgt_txt_processed = tgt_ctx.process(i, src_txt)\n","\n","      if self.__filter(src_txt_processed, tgt_txt_processed): continue\n","      processed.add((src_txt_processed, tgt_txt_processed))\n","\n","    return list(processed)"]}]}