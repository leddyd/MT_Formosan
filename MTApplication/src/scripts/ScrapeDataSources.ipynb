{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["80NJshGP9KCb","tgI3H7QJ9BSN","Z_1HLlZu9w9d","6r46Tw18948G","9K1KIBlh98gd","KsU763YS-i9b","5nz7mfrB_Rje","1DChY8c5_1IX","IESCyY_PAMOM","H3bWT3eMBBZW"],"authorship_tag":"ABX9TyO90dABzhWOLohhuGp65HUJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Scrape Data Sources\n","\n","This script is a combination of multiple different unique scripts used to scrape the data sources used for our model. This script then holds the translation workflow for all files. The data sources scraped in this notebook are\n","\n","Sources scraped direct from web/csv:\n","* Amis ILRDF.org Videos\n","* Paiwan ILRDF.org Videos\n","* Amis Glosbe\n","* Amis Virginia Fey\n","\n","Sources that were transposed from existing XML or .txt files:\n","* Amis ePark\n","* Paiwan ePark\n","* Amis Bible\n","* Amis 2022 Study\n","\n","Sources that were manually aligned:\n","* Amis Apology\n","* Paiwan Apology\n","\n","Hunter Scheppat | scheppat@bc.edu\n"],"metadata":{"id":"vtiSzXrQ8LLt"}},{"cell_type":"markdown","source":["#### Preliminaries"],"metadata":{"id":"80NJshGP9KCb"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install requests beautifulsoup4"],"metadata":{"id":"7c9LY8vQ9Jum"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import requests\n","from bs4 import BeautifulSoup\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QSuubshs9mHs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Scrape the Amis & Paiwan Videos\n","\n","In scraping the Amis/Paiwan videos were decided on just downloading the entire html of the site page. Why? because we could not get our script to work with the 'load more' button on the bottom of the page, which caused us to miss out on scraping lots of videos. This script could probably be improved to work with that.\n","\n","To start, we parse the dowloaned html of the site for all the video IDs, then navigate to each ID and scrape it"],"metadata":{"id":"tgI3H7QJ9BSN"}},{"cell_type":"markdown","source":["#### 1a. Parse HTML"],"metadata":{"id":"Z_1HLlZu9w9d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cgjwkwk28JkJ"},"outputs":[],"source":["from google.colab import drive\n","import requests\n","from bs4 import BeautifulSoup\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Base URL for scraping\n","base_url = 'https://ailt.ilrdf.org.tw/colloquial/'\n","\n","# Set the directory within Google Drive for files and HTML source\n","drive_base_path = '/content/drive/MyDrive/formosan_mt_project/translations/'\n","save_directory = os.path.join(drive_base_path, 'amis_videos')\n","\n","# Ensure the save directory exists\n","if not os.path.exists(save_directory):\n","    os.makedirs(save_directory)\n","\n","# Function to read the HTML file and extract page numbers\n","def get_pages_to_scrape(html_file_path):\n","    with open(html_file_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","    soup = BeautifulSoup(content, 'html.parser')\n","    data_ids = [int(tag['data-id']) for tag in soup.find_all(attrs={'data-id': True})]\n","    print('Total files: ' + str(len(data_ids)))\n","    return data_ids\n","\n","# Function to detect if the page is in reversed format\n","def is_reversed_format(page_number):\n","    # List of known reversed format video page numbers\n","    reversed_pages = [686, 708, 724, 738]\n","    return page_number in reversed_pages\n","\n","# Modified scraping function\n","def scrape_translations(page_number):\n","    ind_filename = os.path.join(save_directory, f\"{page_number}-indigenous.txt\")\n","    zh_filename = os.path.join(save_directory, f\"{page_number}-chinese.txt\")\n","\n","    # Skip if files exist\n","    if os.path.exists(ind_filename) and os.path.exists(zh_filename):\n","        print(f\"Skipping {page_number}: files already exist.\")\n","        return\n","\n","    url = f\"{base_url}{page_number}\"\n","    response = requests.get(url)\n","    if response.status_code != 200:\n","        print(f\"Failed to fetch {url}\")\n","        return\n","\n","    reversed_format = is_reversed_format(page_number)\n","\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    translation_inds = soup.find_all('div', class_='translation_ind')\n","    translation_zhs = soup.find_all('div', class_='translation_zh')\n","\n","    with open(ind_filename, 'w', encoding='utf-8') as ind_file, open(zh_filename, 'w', encoding='utf-8') as zh_file:\n","        for ind_div, zh_div in zip(translation_inds, translation_zhs):\n","            if reversed_format:\n","                # In reversed format, swap the roles of ind and zh\n","                ind_texts = zh_div.get_text(strip=True).split()\n","                zh_line = ind_div.get_text(strip=True)\n","                ind_line = ' '.join(ind_texts)\n","            else:\n","                # Normal format processing\n","                ind_texts = [span.text for span in ind_div.find_all('span', class_='ind_dictionary')]\n","                ind_line = ' '.join(ind_texts)\n","                zh_line = zh_div.get_text(strip=True)\n","\n","            ind_file.write(ind_line + '\\n')\n","            zh_file.write(zh_line + '\\n')\n","\n","def main():\n","    html_file_path = os.path.join(drive_base_path, 'html.txt')\n","    pages_to_scrape = get_pages_to_scrape(html_file_path)\n","    for page_number in pages_to_scrape:\n","        scrape_translations(page_number)\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"markdown","source":["#### 1b. Clean files to remove code switching"],"metadata":{"id":"6r46Tw18948G"}},{"cell_type":"code","source":["# Clean the files and maintain the parralel structure\n","\n","import os\n","import re\n","\n","def contains_english_characters(string):\n","    return bool(re.search('[a-zA-Z]', string))\n","\n","def contains_chinese_characters(string):\n","    return bool(re.search('[\\u4e00-\\u9fff]', string))\n","\n","def process_file_pair(amis_file_path, chinese_file_path):\n","    with open(amis_file_path, 'r', encoding='utf-8') as amis_file, open(chinese_file_path, 'r', encoding='utf-8') as chinese_file:\n","        amis_lines = amis_file.readlines()\n","        chinese_lines = chinese_file.readlines()\n","\n","    assert len(amis_lines) == len(chinese_lines), \"Files are not parallel!\"\n","\n","    # Determine which lines to keep\n","    lines_to_keep = []\n","    for idx, (amis_line, chinese_line) in enumerate(zip(amis_lines, chinese_lines)):\n","        if contains_english_characters(chinese_line) or contains_chinese_characters(amis_line) or (amis_line.strip() == \"\" or chinese_line.strip() == \"\"):\n","            continue  # Skip this line in both files\n","        lines_to_keep.append(idx)\n","\n","    # Rewrite the files without the removed lines\n","    with open(amis_file_path, 'w', encoding='utf-8') as amis_file, open(chinese_file_path, 'w', encoding='utf-8') as chinese_file:\n","        for idx in lines_to_keep:\n","            amis_file.write(amis_lines[idx])\n","            chinese_file.write(chinese_lines[idx])\n","\n","def main():\n","    directory = '/content/drive/MyDrive/formosan_mt_project/translations/amis_videos/'\n","    file_pairs = []\n","\n","    # Assuming file naming convention is consistent and all files are in the same directory\n","    for filename in os.listdir(directory):\n","        if 'indigenous' in filename:\n","            num = filename.split('-')[0]\n","            amis_file_path = os.path.join(directory, f'{num}-indigenous.txt')\n","            chinese_file_path = os.path.join(directory, f'{num}-chinese.txt')\n","            if os.path.exists(chinese_file_path):\n","                file_pairs.append((amis_file_path, chinese_file_path))\n","\n","    for amis_file, chinese_file in file_pairs:\n","        process_file_pair(amis_file, chinese_file)\n","        print(f'Processed {amis_file} and {chinese_file}')\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"l4jpktBb97E1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1c. Validate files are still parallel after cleaning"],"metadata":{"id":"9K1KIBlh98gd"}},{"cell_type":"code","source":["# Validate that files are still parralel after cleaning\n","def validate_file_pairs(directory):\n","    file_pairs = []\n","    for filename in os.listdir(directory):\n","        if 'indigenous' in filename:\n","            num = filename.split('-')[0]\n","            amis_file_path = os.path.join(directory, f'{num}-indigenous.txt')\n","            chinese_file_path = os.path.join(directory, f'{num}-chinese.txt')\n","            if os.path.exists(chinese_file_path):\n","                file_pairs.append((amis_file_path, chinese_file_path))\n","\n","    for amis_file, chinese_file in file_pairs:\n","        with open(amis_file, 'r', encoding='utf-8') as f_amis, open(chinese_file, 'r', encoding='utf-8') as f_chinese:\n","            amis_lines = f_amis.readlines()\n","            chinese_lines = f_chinese.readlines()\n","\n","        if len(amis_lines) != len(chinese_lines):\n","            print(f'Validation failed for files: {amis_file} and {chinese_file}. Line count mismatch: {len(amis_lines)} (Amis) vs {len(chinese_lines)} (Chinese)')\n","        else:\n","            print(f'Validation passed for files: {amis_file} and {chinese_file}. Both files have {len(amis_lines)} lines.')\n","\n","def main():\n","    directory = '/content/drive/MyDrive/formosan_mt_project/translations/amis_videos/'\n","    # Process the files first (your existing processing logic here)\n","\n","    # After processing, validate the files\n","    validate_file_pairs(directory)\n","\n","if __name__ == '__main__':\n","    main()\n","\n"],"metadata":{"id":"ydBaqFx--G0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**THE PROCESS ABOVE CAN BE REPEATED FOR THE PAIWAN VIDEOS**"],"metadata":{"id":"grW67mdP-Xwv"}},{"cell_type":"markdown","source":["## 2. Scrape the Amis Glosbe.com dictionary\n","The Glosbe is very tricky, it doesn't let you view all the sentences, you have to query some word. So we try to query the 10 most common Amis words, get all the sentences, and then remove dupes"],"metadata":{"id":"KsU763YS-i9b"}},{"cell_type":"code","source":["# Top 50 amis words\n","word_freq_list = [('to', 10711), ('i', 10252), ('ko', 10188), ('a', 8331), ('no', 7030), ('o', 5346), ('sa', 3452), ('haw', 3330), ('ku', 3062), ('han', 2867), ('tu', 2765), ('ako', 2564), ('ira', 2501), ('kako', 2426), ('nu', 2137), ('u', 2132), ('ho', 2074), ('hay', 1577), ('ano', 1516), ('kora', 1483), ('caay', 1423), ('itini', 1379), ('itiya', 1350), ('kiya', 1335), ('nira', 1251), ('kami', 1218), ('mako', 1144), ('kira', 1133), ('niyam', 1108), ('saan', 1058), ('wawa', 1019), ('ci', 1017), ('san', 994), ('saka', 980), ('sato', 972), ('ya', 969), ('awaay', 938), ('sanay', 917), ('ka', 898), ('hananay', 884), ('kita', 827), ('mita', 781), ('ta', 771), ('matoÊ¼asay', 768), ('sowal', 761), ('niyaro', 742), ('aca', 730), ('anini', 674), ('tayra', 670), ('ha', 665)]\n","top_words = {word for word, freq in word_freq_list}"],"metadata":{"id":"hY7E3umm-xRt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Query these and save"],"metadata":{"id":"iBXkU3T8_K3F"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Initialize base URL and top words\n","base_url = \"https://glosbe.com/ami/zh/\"\n","\n","# Paths for the output files\n","indigenous_path = '/content/drive/MyDrive/formosan_mt_project/translations/amis_glosbe/glosbe-indigenous.txt'\n","chinese_path = '/content/drive/MyDrive/formosan_mt_project/translations/amis_glosbe/glosbe-chinese.txt'\n","\n","# Helper function to save sentences to files ensuring parallel corpus structure\n","def save_sentences(indigenous_sentences, chinese_sentences):\n","    with open(indigenous_path, 'a', encoding='utf-8') as fi, open(chinese_path, 'a', encoding='utf-8') as fc:\n","        for ind, chi in zip(indigenous_sentences, chinese_sentences):\n","            fi.write(ind + '\\n')\n","            fc.write(chi + '\\n')\n","\n","# Function to process each word and maintain parallel structure\n","def scrape_for_word(word):\n","    url = base_url + word\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Initialize data structures to track sentences and ensure uniqueness\n","    seen_pairs = set()\n","    indigenous_sentences = []\n","    chinese_sentences = []\n","\n","    # Internal function to process a page\n","    def process_page(soup):\n","        divs = soup.find_all('div', class_='flex')\n","        for div in divs:\n","            ami_text = div.find('div', attrs={'lang': 'ami'})\n","            zh_text = div.find('div', attrs={'lang': 'zh'})\n","            if ami_text and zh_text:\n","                ami_sentence = ami_text.text.strip()\n","                zh_sentence = zh_text.text.strip()\n","                pair = (ami_sentence, zh_sentence)\n","                if pair not in seen_pairs:\n","                    seen_pairs.add(pair)\n","                    indigenous_sentences.append(ami_sentence)\n","                    chinese_sentences.append(zh_sentence)\n","\n","    process_page(soup)\n","\n","    # Manage \"Load More\" functionality\n","    load_more = soup.find('button', attrs={'data-element': 'fragment-loader'})\n","    while load_more:\n","        more_url = 'https://glosbe.com' + load_more['data-fragment-url']\n","        response = requests.get(more_url)\n","        more_soup = BeautifulSoup(response.text, 'html.parser')\n","        process_page(more_soup)\n","        load_more = more_soup.find('button', attrs={'data-element': 'fragment-loader'})\n","\n","    # Save the sentences ensuring parallel structure\n","    save_sentences(indigenous_sentences, chinese_sentences)\n","\n","# Processing each word\n","for word in top_words:\n","    scrape_for_word(word)\n","\n","print(\"Scraping complete. Files saved.\")\n"],"metadata":{"id":"cPPet9VK_KEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Scrape the Amis Virginia Fey\n","\n","This part was fairly simple, as it was already in a csv"],"metadata":{"id":"5nz7mfrB_Rje"}},{"cell_type":"code","source":["# Path to your CSV file\n","csv_file_path = '/content/drive/MyDrive/formosan_mt_project/translations/virginia_fey.csv'\n","\n","# Paths where the text files will be saved\n","indigenous_file_path = '/content/drive/MyDrive/formosan_mt_project/translations/virginia-fey-indigenous.txt'\n","english_file_path = '/content/drive/MyDrive/formosan_mt_project/translations/virginia-fey-english.txt'\n","\n","# Read the CSV file\n","df = pd.read_csv(csv_file_path)\n","\n","# Assuming column 'a' contains indigenous sentences and column 'b' contains English sentences\n","# Check if column names are correct, adjust 'a' and 'b' if needed\n","indigenous_sentences = df['Amis']\n","english_sentences = df['English']\n","\n","# Save indigenous sentences to a text file\n","with open(indigenous_file_path, 'w', encoding='utf-8') as f:\n","    for sentence in indigenous_sentences:\n","        f.write(sentence + '\\n')\n","\n","# Save English sentences to a text file\n","with open(english_file_path, 'w', encoding='utf-8') as f:\n","    for sentence in english_sentences:\n","        f.write(sentence + '\\n')\n","\n","print(\"Files have been saved successfully.\")"],"metadata":{"id":"DFUTBPKy_U_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"h5nqOz27_idQ"}},{"cell_type":"markdown","source":["## 4. Scrape the Amis & Paiwan ePark\n","\n","These files were already in XML, so we took them out of XML format"],"metadata":{"id":"1DChY8c5_1IX"}},{"cell_type":"code","source":["import os\n","import xml.etree.ElementTree as ET\n","import random\n","\n","def generate_unique_id(used_ids):\n","    while True:\n","        unique_id = random.randint(100, 999)\n","        if unique_id not in used_ids:\n","            used_ids.add(unique_id)\n","            return unique_id\n","\n","def parse_and_save(xml_file_path, save_directory, used_ids):\n","    unique_id = generate_unique_id(used_ids)\n","    root_name = os.path.splitext(os.path.basename(xml_file_path))[0]\n","    amis_file_path = os.path.join(save_directory, f\"{unique_id}-{root_name}-paiwan.txt\")\n","    english_file_path = os.path.join(save_directory, f\"{unique_id}-{root_name}-english.en\")\n","\n","    tree = ET.parse(xml_file_path)\n","    root = tree.getroot()\n","    namespaces = {'xml': 'http://www.w3.org/XML/1998/namespace'}\n","\n","    os.makedirs(save_directory, exist_ok=True)\n","\n","    with open(amis_file_path, 'w', encoding='utf-8') as amis_file, \\\n","            open(english_file_path, 'w', encoding='utf-8') as english_file:\n","\n","            for line_number, s in enumerate(root.findall('.//S')):\n","                form = s.find('FORM')\n","                english = s.find('.//TRANSL[@xml:lang=\"en\"]', namespaces=namespaces)\n","\n","                if form is not None and english is not None and form.text and english.text:\n","                    amis_text = form.text.strip()\n","                    english_text = english.text.strip()\n","\n","                    if amis_text and english_text:\n","                        # Modification: Write only if content exists\n","                        amis_file.write(amis_text + \"\\n\")\n","                        english_file.write(english_text + \"\\n\")\n","                    else:\n","                        print(f\"Skipped empty text entry at S ID {s.attrib['id']}\")\n","                else:\n","                    print(f\"Skipped due to missing translation at S ID {s.attrib['id']}\")\n","\n","    # Cross-Check for Line Correspondence\n","    with open(amis_file_path) as amis_file, open(english_file_path) as english_file:\n","        amis_lines = amis_file.readlines()\n","        english_lines = english_file.readlines()\n","\n","        if len(amis_lines) != len(english_lines):\n","            print(f\"Error: Line mismatch in {amis_file_path} and {english_file_path}\")\n","        else:\n","            for line_number, (amis_line, english_line) in enumerate(zip(amis_lines, english_lines)):\n","                if amis_line.strip() != english_line.strip():\n","                    print(f\"Error: Mismatch at Line {line_number + 1} in files {amis_file_path} and {english_file_path}\")\n","                    break  # Stop after reporting the first mismatch\n","\n","    print(f\"Written files: {amis_file_path} and {english_file_path}\")\n","\n","\n","def process_directory(directory_path, save_directory):\n","    for file_name in os.listdir(directory_path):\n","        if file_name.endswith('.xml'):\n","            file_path = os.path.join(directory_path, file_name)\n","            parse_and_save(file_path, save_directory, used_ids)\n","\n","# Example directory structure and paths\n","directories = [\n","    '/content/drive/MyDrive/formosan_mt_project/xml/paiwan_videos/',\n","]\n","save_directory = '/content/drive/MyDrive/formosan_mt_project/translations/paiwan/paiwan_videos/'\n","used_ids = set()\n","\n","for directory in directories:\n","    process_directory(directory, save_directory)\n","\n","print(\"Processing complete.\")\n"],"metadata":{"id":"orW-wMDd_8mM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**THIS CODE CAN BE RESUED FOR ANY XML, JUST CHANGE THE LANGUAGE TAG**"],"metadata":{"id":"up3Lpz45AC7K"}},{"cell_type":"markdown","source":["## 5. Scrape the 2022 Study\n","\n","This study was already aligned but the dictionary was not parallel yet"],"metadata":{"id":"IESCyY_PAMOM"}},{"cell_type":"markdown","source":["#### Split the dictionary portion, which was individual words"],"metadata":{"id":"S6c1tdxAAbGz"}},{"cell_type":"code","source":["# Path to the Amis -> Chinese dictionary file\n","current_dict = '/content/gdrive/MyDrive/formosan_mt_project/amis_dictionary/ami-cmn.txt'\n","\n","# Directory to save the output files\n","save_location = '/content/gdrive/MyDrive/formosan_mt_project/translations/amis/dictionary'\n","\n","# Make sure the save directory exists\n","import os\n","if not os.path.exists(save_location):\n","    os.makedirs(save_location)\n","\n","# Filenames for the output Amis and Chinese files\n","amis_file_path = os.path.join(save_location, 'amis-dict.txt')\n","chinese_file_path = os.path.join(save_location, 'chinese-dict.txt')\n","\n","# Read the dictionary file and split into separate Amis and Chinese files\n","with open(current_dict, 'r', encoding='utf-8') as file, \\\n","     open(amis_file_path, 'w', encoding='utf-8') as amis_file, \\\n","     open(chinese_file_path, 'w', encoding='utf-8') as chinese_file:\n","\n","    for line in file:\n","        # Split the line into Amis word and Chinese translation\n","        # Assuming there is a consistent separator such as multiple spaces\n","        parts = line.strip().split(maxsplit=1)\n","        if len(parts) == 2:\n","            amis_word, chinese_translation = parts\n","            # Write to respective files\n","            amis_file.write(amis_word + '\\n')\n","            chinese_file.write(chinese_translation + '\\n')\n","\n","print(\"Splitting complete! Files saved at:\", save_location)\n"],"metadata":{"id":"4dYITgt7ASaS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Translate using DeepL\n","\n","In this script we translate by calling the DeepL api, this script can be modified to work with pretty much any data source"],"metadata":{"id":"H3bWT3eMBBZW"}},{"cell_type":"code","source":["!pip install deepl"],"metadata":{"id":"VEhXcjC5BJnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Translate with DeepL\n","import os\n","import deepl\n","\n","# Replace with the path to your directory containing the files to be translated\n","directory_path = '/content/gdrive/MyDrive/formosan_mt_project/translations/amis/dictionary/'\n","\n","# DO NOT TAKE MY API KEY PLEASE!!! :)\n","auth_key = \"64b1b011-8415-4f50-b3f1-ba4def3a4374:fx\"\n","translator = deepl.Translator(auth_key)\n","\n","def translate_file(file_path):\n","    base_name = os.path.basename(file_path)\n","    file_number = base_name.split('-')[0]  # Assumes the file name format is \"number-chinese.txt\"\n","    output_file_name = f\"{file_number}-english.txt\"\n","    output_file_path = os.path.join(directory_path, output_file_name)\n","\n","    # Check if the English translation file already exists\n","    if os.path.exists(output_file_path):\n","        print(f\"Skipping translation for {file_number}: English file already exists.\")\n","        return\n","\n","    lines_to_translate = []\n","    with open(file_path, 'r', encoding='utf-8') as input_file:\n","        lines_to_translate = [line.strip() for line in input_file.readlines() if line.strip()]  # Skip empty lines\n","\n","    # DeepL API supports up to 50 texts in one request\n","    batch_size = 50\n","    translated_lines = []\n","    for i in range(0, len(lines_to_translate), batch_size):\n","        batch = lines_to_translate[i:i+batch_size]\n","        results = translator.translate_text(batch, source_lang=\"ZH\", target_lang=\"EN-US\")\n","        translated_lines.extend([result.text for result in results])\n","\n","    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n","        output_file.write('\\n'.join(translated_lines))\n","\n","for file in os.listdir(directory_path):\n","    if file.endswith(\"-chinese.zh\"):\n","        file_path = os.path.join(directory_path, file)\n","        print(f\"Processing {file}...\")\n","        translate_file(file_path)\n","        print(f\"Finished processing {file}.\")\n"],"metadata":{"id":"fn91AVMaBDoz"},"execution_count":null,"outputs":[]}]}