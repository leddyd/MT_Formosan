{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40229,"status":"ok","timestamp":1714666124879,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"},"user_tz":420},"id":"HCgqyH3nA6ga","outputId":"24141e73-c267-42b6-9fc9-cd4e98fde518"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu  -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaPnLdwjBB_M"},"outputs":[],"source":["import gc\n","import random\n","import numpy as np\n","import torch\n","import pandas as pd\n","import sacrebleu\n","import sentencepiece as spm\n","from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n","from transformers import NllbTokenizer\n","from transformers import Adafactor, get_scheduler\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import get_constant_schedule_with_warmup\n","from tqdm.auto import tqdm, trange\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oH9Ovt3dAsHD"},"outputs":[],"source":["class Translator:\n","  def __init__(self, learningRate=1e-3, batchSize: int=16):\n","    self.tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')\n","    self.__fix_tokenizer()\n","    self.model = AutoModelForSeq2SeqLM.from_pretrained('facebook/nllb-200-distilled-600M')\n","    self.model.resize_token_embeddings(len(self.tokenizer))\n","    self.learningRate = learningRate\n","    self.batchSize = batchSize\n","    self.losses = []\n","    self.LANGS = [('eng', 'eng_Latn'), ('ami', 'ami_Latn')]\n","\n","\n","  def __setup(self):\n","    self.optimizer = Adafactor(\n","        [p for p in self.model.parameters() if p.requires_grad],\n","        scale_parameter=False,\n","        relative_step=False,\n","        lr=self.learningRate,  # Starting with a higher learning rate\n","        clip_threshold=1.0,\n","        weight_decay=1e-3,\n","      )\n","\n","    self.scheduler = get_constant_schedule_with_warmup(\n","        self.optimizer,\n","        num_warmup_steps=500\n","      )\n","\n","\n","  def __word_tokenize(text: str):\n","    return re.findall('(\\w+|[^\\w\\s])', text)\n","\n","\n","  def __generate_splits(self, data: object, split: bool):\n","    data.rename(columns={'ami': 'Amis', 'eng': 'English'}, inplace=True)\n","\n","    if split:\n","      self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n","          data['Amis'],\n","          data['English'],\n","          random_state=104,\n","          test_size=0.2,\n","          shuffle=True\n","        )\n","    else:\n","      self.x_train = data[data['split'] == 'train']['Amis']\n","      self.y_train = data[data['split'] == 'train']['English']\n","      self.x_test = data[data['split'] == 'test']['Amis']\n","      self.y_test = data[data['split'] == 'test']['English']\n","\n","    self.y_train['eng_toks'] = self.y_train.English.apply(self.tokenizer.tokenize)\n","    self.x_train['ami_toks'] = self.x_train.Amis.apply(self.tokenizer.tokenize)\n","    self.y_train['eng_words'] = self.y_train.English.apply(self.word_tokenize)\n","    self.x_train['ami_words'] = self.x_train.Amis.apply(self.word_tokenize)\n","\n","\n","  def __fix_tokenizer(self, new_lang: str='ami_Latn'):\n","    old_len = len(self.tokenizer) - int(new_lang in self.tokenizer.added_tokens_encoder)\n","    self.tokenizer.lang_code_to_id[new_lang] = old_len-1\n","    self.tokenizer.id_to_lang_code[old_len-1] = new_lang\n","    # always move \"mask\" to the last position\n","    self.tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(self.tokenizer.sp_model) + len(self.tokenizer.lang_code_to_id) + self.tokenizer.fairseq_offset\n","\n","    self.tokenizer.fairseq_tokens_to_ids.update(self.tokenizer.lang_code_to_id)\n","    self.tokenizer.fairseq_ids_to_tokens = {v: k for k, v in self.tokenizer.fairseq_tokens_to_ids.items()}\n","    if new_lang not in self.tokenizer._additional_special_tokens:\n","        self.tokenizer._additional_special_tokens.append(new_lang)\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    self.tokenizer.added_tokens_encoder = {}\n","    self.tokenizer.added_tokens_decoder = {}\n","\n","\n","  def __cleanup(self):\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","  def __get_batch_pairs(self):\n","    xx, yy = [], []\n","    for _ in range(self.batchSize):\n","        src = self.x_train.iloc[random.randint(0, len(self.x_train)-1)]\n","        tgt = self.y_train.iloc[random.randint(0, len(self.y_train)-1)]\n","        xx.append(src)\n","        yy.append(tgt)\n","    return xx, yy, 'ami_Latn', 'eng_Latn'\n","\n","\n","  def __tokenize_and_transition(self, lang):\n","    return self.tokenizer(\n","        lang,\n","        return_tensors='pt',\n","        padding=True,\n","        truncation=True,\n","        max_length=128\n","      ).to(self.model.device)\n","\n","\n","  def __train_sentencepiece(self, text_file: str, spm_prefix: str):\n","      spm.SentencePieceTrainer.train(\n","          input=text_file,\n","          model_prefix=spm_prefix,\n","          vocab_size=2**14,  # 16K\n","          character_coverage=1,\n","          num_threads=16,\n","          train_extremely_large_corpus=False,\n","          add_dummy_prefix=False,\n","          max_sentencepiece_length=128,\n","          max_sentence_length=4192*4,\n","          pad_id=0,\n","          eos_id=1,\n","          unk_id=2,\n","          bos_id=-1,\n","      )\n","\n","\n","  def __load_sentencepiece_model(self, model_file: str):\n","      sp_trained = spm.SentencePieceProcessor(model_file=model_file)\n","      added_spm = sp_pb2_model.ModelProto()\n","      added_spm.ParseFromString(sp_trained.serialized_model_proto())\n","      return added_spm\n","\n","\n","  def __update_existing_model_with_new_tokens(self, added_spm: sp_pb2_model.ModelProto):\n","      old_spm = sp_pb2_model.ModelProto()\n","      old_spm.ParseFromString(self.tokenizer.sp_model.serialized_model_proto())\n","\n","      nllb_tokens_set = {p.piece for p in old_spm.pieces}\n","      prev_min_score = old_spm.pieces[-1].score\n","\n","      for p in added_spm.pieces:\n","          piece = p.piece\n","          if piece not in nllb_tokens_set:\n","              new_p = sp_pb2_model.ModelProto().SentencePiece()\n","              new_p.piece = piece\n","              new_p.score = p.score + prev_min_score\n","              old_spm.pieces.append(new_p)\n","\n","      return old_spm\n","\n","\n","  def __save_updated_model(self, updated_spm: sp_pb2_model.ModelProto, new_spm_name: str):\n","      with open(new_spm_name, 'wb') as f:\n","          f.write(updated_spm.SerializeToString())\n","\n","\n","  def __update_tokenizer_and_model(self, new_spm_name: str):\n","      tokenizer_old = self.tokenizer\n","      self.tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M', vocab_file=new_spm_name)\n","      self.model.resize_token_embeddings(len(self.tokenizer))\n","\n","      added_vocab = set(self.tokenizer.get_vocab()).difference(set(tokenizer_old.get_vocab()))\n","\n","      for t in tqdm(added_vocab):\n","          tt = tokenizer_old(t, add_special_tokens=False).input_ids\n","          if len(tt) == 0:\n","              tt = [tokenizer_old.unk_token_id]\n","          idx = self.tokenizer.convert_tokens_to_ids(t)\n","          self.model.model.shared.weight.data[idx] = self.model.model.shared.weight.data[tt].mean(0)\n","\n","\n","  def expand_vocab(self, text_file: str):\n","        spm_prefix = 'spm_amis_16k'\n","        self.__train_sentencepiece(text_file, spm_prefix)\n","        new_spm_model = self.__load_sentencepiece_model(f'{spm_prefix}.model')\n","        updated_spm_model = self.__update_existing_model_with_new_tokens(new_spm_model)\n","        new_spm_name = 'spm_nllb_amis_268k.model'\n","        self.__save_updated_model(updated_spm_model, new_spm_name)\n","        self.__update_tokenizer_and_model(new_spm_name)\n","\n","\n","  # Unfinished\n","  def evaluate(self):\n","    pd.Series(self.losses).ewm(100).mean().plot()\n","\n","\n","  def train(self, data: object, split: bool=False,\n","            epochs: int=25000, save_interval: int=5000,\n","            save_to: str=\"'/content/drive/MyDrive/MTApplication/models/nllb-eng-ami-v1'\"):\n","\n","    self.__setup()\n","    self.__generate_splits(data, split)\n","    self.__cleanup()\n","    self.model.cuda();\n","    self.model.train()\n","\n","    for i in tqdm(range(epochs)):\n","      xx, yy, lang1, lang2 = self.__get_batch_pairs()\n","      try:\n","          # Tokenization and moving tensors to GPU\n","          self.tokenizer.src_lang = lang1\n","          x = self.__tokenize_and_transition(xx)\n","          y = self.__tokenize_and_transition(yy)\n","          self.tokenizer.tgt_lang = lang2\n","\n","          y.input_ids[y.input_ids == self.tokenizer.pad_token_id] = -100\n","\n","          # Forward and backward passes\n","          self.optimizer.zero_grad()\n","          loss = self.model(**x, labels=y.input_ids).loss\n","          loss.backward()\n","          self.optimizer.step()\n","          self.scheduler.step()\n","\n","          self.losses.append(loss.item())\n","          if i % 500 == 0:\n","              print(f\"Step {i}, Loss: {loss.item()}\")\n","\n","          # Checkpoint saving\n","          if i % save_interval == 0 and i > 0:\n","              current_loss_avg = np.mean(self.losses[-1000:])\n","              print(f\"Average Loss last 1000 steps: {current_loss_avg}\")\n","              self.model.save_pretrained(f'{self.MODEL_SAVE_PATH}/checkpoint_{i}')\n","              self.tokenizer.save_pretrained(f'{self.MODEL_SAVE_PATH}/checkpoint_{i}')\n","\n","      except RuntimeError as e:\n","          self.optimizer.zero_grad()\n","          self.cleanup()\n","          print('RuntimeError:', e)\n","          continue\n","\n","      except Exception as e:\n","          self.optimizer.zero_grad()\n","          print('An error occurred:', e)\n","          break\n","\n","    self.model.save_pretrained(f'{save_to}/final')\n","    self.tokenizer.save_pretrained(f'{save_to}/final')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}