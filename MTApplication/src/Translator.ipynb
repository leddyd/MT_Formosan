{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVe9Hx7v/r6Fu6iyKNrFvs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu  -q"],"metadata":{"id":"HCgqyH3nA6ga","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713985349794,"user_tz":240,"elapsed":46663,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"outputId":"2bd79363-5942-4d15-b634-5307c44be8ae"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import gc\n","import random\n","import numpy as np\n","import torch\n","import pandas as pd\n","from transformers import NllbTokenizer\n","from transformers import Adafactor, get_scheduler\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import get_constant_schedule_with_warmup\n","from tqdm.auto import tqdm, trange\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"xaPnLdwjBB_M","executionInfo":{"status":"ok","timestamp":1713985706642,"user_tz":240,"elapsed":144,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WOF6al7Mfp7P","executionInfo":{"status":"ok","timestamp":1713985491496,"user_tz":240,"elapsed":20277,"user":{"displayName":"Dylan Leddy","userId":"11624890938076538258"}},"outputId":"d1249dcd-3859-493b-cde7-c55adf52fcb1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oH9Ovt3dAsHD"},"outputs":[],"source":["class Translator:\n","\n","  def __init__(self, learningRate=1e-3, batchSize=16):\n","    self.tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')\n","    self.__fix_tokenizer()\n","    self.model = AutoModelForSeq2SeqLM.from_pretrained('facebook/nllb-200-distilled-600M')\n","    self.model.resize_token_embeddings(len(self.tokenizer))\n","    self.learningRate = learningRate\n","    self.batchSize = batchSize\n","    self.losses = []\n","    self.LANGS = [('eng', 'eng_Latn'), ('ami', 'ami_Latn')]\n","\n","\n","  def __setup(self):\n","    self.optimizer = Adafactor(\n","        [p for p in self.model.parameters() if p.requires_grad],\n","        scale_parameter=False,\n","        relative_step=False,\n","        lr=self.learningRate,  # Starting with a higher learning rate\n","        clip_threshold=1.0,\n","        weight_decay=1e-3,\n","      )\n","\n","    self.scheduler = get_constant_schedule_with_warmup(\n","        self.optimizer,\n","        num_warmup_steps=500\n","      )\n","\n","\n","  def __generate_splits(self, data):\n","    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n","        data['Amis'],\n","        data['English'],\n","        random_state=104,\n","        test_size=0.2,\n","        shuffle=True\n","      )\n","\n","\n","  def __fix_tokenizer(self, new_lang='ami_Latn'):\n","    old_len = len(self.tokenizer) - int(new_lang in self.tokenizer.added_tokens_encoder)\n","    self.tokenizer.lang_code_to_id[new_lang] = old_len-1\n","    self.tokenizer.id_to_lang_code[old_len-1] = new_lang\n","    # always move \"mask\" to the last position\n","    self.tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(self.tokenizer.sp_model) + len(self.tokenizer.lang_code_to_id) + self.tokenizer.fairseq_offset\n","\n","    self.tokenizer.fairseq_tokens_to_ids.update(self.tokenizer.lang_code_to_id)\n","    self.tokenizer.fairseq_ids_to_tokens = {v: k for k, v in self.tokenizer.fairseq_tokens_to_ids.items()}\n","    if new_lang not in self.tokenizer._additional_special_tokens:\n","        self.tokenizer._additional_special_tokens.append(new_lang)\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    self.tokenizer.added_tokens_encoder = {}\n","    self.tokenizer.added_tokens_decoder = {}\n","\n","\n","  def __cleanup(self):\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","  def __get_batch_pairs(self):\n","    xx, yy = [], []\n","    for _ in range(self.batchSize):\n","        src = self.x_train.iloc[random.randint(0, len(self.x_train)-1)]\n","        tgt = self.y_train.iloc[random.randint(0, len(self.y_train)-1)]\n","        xx.append(src)\n","        yy.append(tgt)\n","    return xx, yy, 'ami_Latn', 'eng_Latn'\n","\n","\n","  def __tokenize_and_transition(self, lang):\n","    return self.tokenizer(\n","        lang,\n","        return_tensors='pt',\n","        padding=True,\n","        truncation=True,\n","        max_length=128\n","      ).to(self.model.device)\n","\n","\n","  def evaluate(self):\n","    pd.Series(self.losses).ewm(100).mean().plot()\n","\n","\n","  def train(self, data, epochs: int=50000, save_interval: int=5000, save_to: str=\"'/content/drive/MyDrive/MTApplication/models/nllb-eng-ami-v1'\"):\n","    self.__setup()\n","    self.__generate_splits(data)\n","    self.__cleanup()\n","    self.model.cuda();\n","    self.model.train()\n","\n","    for i in tqdm(range(epochs)):\n","      xx, yy, lang1, lang2 = self.__get_batch_pairs()\n","      try:\n","          # Tokenization and moving tensors to GPU\n","          self.tokenizer.src_lang = lang1\n","          x = self.__tokenize_and_transition(xx)\n","          y = self.__tokenize_and_transition(yy)\n","          self.tokenizer.tgt_lang = lang2\n","\n","          y.input_ids[y.input_ids == self.tokenizer.pad_token_id] = -100\n","\n","          # Forward and backward passes\n","          self.optimizer.zero_grad()\n","          loss = self.model(**x, labels=y.input_ids).loss\n","          loss.backward()\n","          self.optimizer.step()\n","          self.scheduler.step()\n","\n","          self.losses.append(loss.item())\n","          if i % 500 == 0:\n","              print(f\"Step {i}, Loss: {loss.item()}\")\n","\n","          # Checkpoint saving\n","          if i % save_interval == 0 and i > 0:\n","              current_loss_avg = np.mean(self.losses[-1000:])\n","              print(f\"Average Loss last 1000 steps: {current_loss_avg}\")\n","              self.model.save_pretrained(f'{self.MODEL_SAVE_PATH}/checkpoint_{i}')\n","              self.tokenizer.save_pretrained(f'{self.MODEL_SAVE_PATH}/checkpoint_{i}')\n","\n","      except RuntimeError as e:\n","          self.optimizer.zero_grad()\n","          self.cleanup()\n","          print('RuntimeError:', e)\n","          continue\n","\n","      except Exception as e:\n","          self.optimizer.zero_grad()\n","          print('An error occurred:', e)\n","          break\n","\n","    self.model.save_pretrained(f'{save_to}/final')\n","    self.tokenizer.save_pretrained(f'{save_to}/final')\n","\n","\n","    #def translate(self, source: str):\n"]}]}